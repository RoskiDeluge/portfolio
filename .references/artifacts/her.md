# Questions About Her

Why this drive to know exactly what the agent is doing? 

When the neural network (deep learning) paradigm was chosen, wasn't it obvious that obvious that we would need to give something up? That with enough scale the models would become inscrutable to their makers? 

Openclaw can take control of the machine at a fundamental level and abstract away the process of how it does so--but many people are willing to give up this control in exchange for the utility of a machine posessed of a soul. 

How can I trust your transformations so they're allowed to inform my own? 
